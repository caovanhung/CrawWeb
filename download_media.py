#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import json
import os
import requests
import ssl
from urllib.parse import urljoin, urlparse
import re
from datetime import datetime
import time

# T·∫Øt SSL verification
ssl._create_default_https_context = ssl._create_unverified_context

class MediaDownloader:
    def __init__(self, json_file='output/articles.json'):
        self.json_file = json_file
        self.base_dir = 'output/media_articles'
        self.session = requests.Session()
        self.session.verify = False
        
        # Headers ƒë·ªÉ gi·∫£ l·∫≠p browser
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'vi-VN,vi;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
        }
        
        # T·∫°o th∆∞ m·ª•c g·ªëc
        os.makedirs(self.base_dir, exist_ok=True)
    
    def sanitize_filename(self, filename):
        """L√†m s·∫°ch t√™n file ƒë·ªÉ tr√°nh l·ªói"""
        # Lo·∫°i b·ªè c√°c k√Ω t·ª± kh√¥ng h·ª£p l·ªá
        filename = re.sub(r'[<>:"/\\|?*]', '_', filename)
        # Gi·ªõi h·∫°n ƒë·ªô d√†i
        if len(filename) > 200:
            filename = filename[:200]
        return filename
    
    def download_file(self, url, filepath):
        """T·∫£i file t·ª´ URL"""
        try:
            response = self.session.get(url, headers=self.headers, timeout=30)
            response.raise_for_status()
            
            with open(filepath, 'wb') as f:
                f.write(response.content)
            return True
        except Exception as e:
            print(f"  L·ªói t·∫£i {url}: {e}")
            return False
    
    def extract_images_from_html(self, html_content, article_url):
        """Tr√≠ch xu·∫•t ·∫£nh t·ª´ HTML c·ªßa b√†i vi·∫øt (ch·ªâ ·∫£nh trong n·ªôi dung)"""
        images = []
        
        # T√¨m ·∫£nh trong c√°c th·∫ª img
        img_patterns = [
            r'<img[^>]*src="([^"]*)"[^>]*alt="([^"]*)"[^>]*>',
            r'<img[^>]*alt="([^"]*)"[^>]*src="([^"]*)"[^>]*>',
            r'<img[^>]*src="([^"]*)"[^>]*>',
        ]
        
        for pattern in img_patterns:
            matches = re.findall(pattern, html_content, re.IGNORECASE)
            for match in matches:
                if len(match) == 2:
                    img_url, img_alt = match
                else:
                    img_url = match[0]
                    img_alt = f"image_{len(images)}"
                
                if img_url and self.is_content_image(img_url, img_alt):
                    # Chuy·ªÉn URL t∆∞∆°ng ƒë·ªëi th√†nh tuy·ªát ƒë·ªëi
                    if img_url.startswith('/'):
                        img_url = urljoin(article_url, img_url)
                    elif not img_url.startswith('http'):
                        img_url = urljoin(article_url, img_url)
                    
                    images.append({
                        'url': img_url,
                        'alt': img_alt,
                        'type': 'content_image'
                    })
        
        return list({img['url']: img for img in images}.values())  # Lo·∫°i b·ªè tr√πng l·∫∑p
    
    def is_content_image(self, img_url, img_alt):
        """Ki·ªÉm tra xem ·∫£nh c√≥ ph·∫£i l√† ·∫£nh n·ªôi dung b√†i vi·∫øt kh√¥ng"""
        # Lo·∫°i b·ªè ·∫£nh qu·∫£ng c√°o
        ad_keywords = [
            'quang-cao', 'advertisement', 'ads', 'banner', 'sponsor',
            'qu·∫£ng c√°o', 'banner', 'sponsor', 'ad', 'ads'
        ]
        
        img_url_lower = img_url.lower()
        img_alt_lower = img_alt.lower()
        
        # Ki·ªÉm tra URL v√† alt text c√≥ ch·ª©a t·ª´ kh√≥a qu·∫£ng c√°o
        for keyword in ad_keywords:
            if keyword in img_url_lower or keyword in img_alt_lower:
                return False
        
        # Lo·∫°i b·ªè ·∫£nh c√≥ k√≠ch th∆∞·ªõc nh·ªè (th∆∞·ªùng l√† icon, avatar)
        size_patterns = [
            r'(\d+)x(\d+)',  # 100x100, 200x200
            r'thumb', 'thumbnail', 'icon', 'avatar'
        ]
        
        for pattern in size_patterns:
            if re.search(pattern, img_url_lower):
                # Ki·ªÉm tra n·∫øu l√† ·∫£nh nh·ªè th√¨ b·ªè qua
                if 'thumb' in img_url_lower or 'icon' in img_url_lower or 'avatar' in img_url_lower:
                    return False
        
        # Ch·ªâ l·∫•y ·∫£nh t·ª´ domain ch√≠nh c·ªßa baotintuc.vn
        valid_domains = [
            'baotintuc.vn',
            'cdnmedia.baotintuc.vn',
            'cdnthumb.baotintuc.vn',
            'media.baotintuc.vn'
        ]
        
        is_valid_domain = any(domain in img_url_lower for domain in valid_domains)
        
        return is_valid_domain
    
    def extract_videos_from_html(self, html_content):
        """Tr√≠ch xu·∫•t video URLs t·ª´ HTML (ch·ªâ video trong n·ªôi dung)"""
        videos = []
        
        # T√¨m c√°c th·∫ª video trong n·ªôi dung b√†i vi·∫øt
        video_patterns = [
            r'<video[^>]*src="([^"]*)"[^>]*>',
            r'<video[^>]*><source[^>]*src="([^"]*)"[^>]*>',
            r'<iframe[^>]*src="([^"]*)"[^>]*>',
            r'data-video-url="([^"]*)"',
            r'video-url="([^"]*)"',
        ]
        
        for pattern in video_patterns:
            matches = re.findall(pattern, html_content, re.IGNORECASE)
            for match in matches:
                if match and self.is_content_video(match):
                    videos.append(match)
        
        return list(set(videos))  # Lo·∫°i b·ªè tr√πng l·∫∑p
    
    def is_content_video(self, video_url):
        """Ki·ªÉm tra xem video c√≥ ph·∫£i l√† video n·ªôi dung b√†i vi·∫øt kh√¥ng"""
        video_url_lower = video_url.lower()
        
        # Lo·∫°i b·ªè video qu·∫£ng c√°o
        ad_keywords = [
            'quang-cao', 'advertisement', 'ads', 'banner', 'sponsor',
            'qu·∫£ng c√°o', 'banner', 'sponsor', 'ad', 'ads'
        ]
        
        for keyword in ad_keywords:
            if keyword in video_url_lower:
                return False
        
        # Ch·ªâ l·∫•y video t·ª´ c√°c ngu·ªìn h·ª£p l·ªá
        valid_sources = [
            'baotintuc.vn',
            'youtube.com',
            'youtu.be',
            'vimeo.com',
            'dailymotion.com'
        ]
        
        return any(source in video_url_lower for source in valid_sources)
    
    def process_article(self, article, index):
        """X·ª≠ l√Ω m·ªôt b√†i vi·∫øt"""
        title = article.get('title', f'B√†i vi·∫øt {index}')
        url = article.get('url', '')
        
        print(f"\n[{index}] ƒêang x·ª≠ l√Ω: {title[:50]}...")
        
        # T·∫°o t√™n th∆∞ m·ª•c t·ª´ title
        safe_title = self.sanitize_filename(title)
        article_dir = os.path.join(self.base_dir, f"{index:02d}_{safe_title}")
        os.makedirs(article_dir, exist_ok=True)
        
        # L∆∞u th√¥ng tin b√†i vi·∫øt
        article_info = {
            'title': title,
            'url': url,
            'summary': article.get('summary', ''),
            'date': article.get('date', ''),
            'topic': article.get('topic', ''),
            'content': article.get('content', '')[:500] + '...' if len(article.get('content', '')) > 500 else article.get('content', '')
        }
        
        with open(os.path.join(article_dir, 'article_info.json'), 'w', encoding='utf-8') as f:
            json.dump(article_info, f, ensure_ascii=False, indent=2)
        
        # T·∫£i HTML v√† tr√≠ch xu·∫•t ·∫£nh/video t·ª´ n·ªôi dung b√†i vi·∫øt
        images = []
        videos = []
        
        if url:
            try:
                print(f"  T·∫£i HTML ƒë·ªÉ tr√≠ch xu·∫•t ·∫£nh v√† video...")
                response = self.session.get(url, headers=self.headers, timeout=30)
                response.raise_for_status()
                
                html_content = response.text
                
                # L∆∞u HTML
                with open(os.path.join(article_dir, 'article.html'), 'w', encoding='utf-8') as f:
                    f.write(html_content)
                
                # Tr√≠ch xu·∫•t ·∫£nh t·ª´ n·ªôi dung b√†i vi·∫øt
                images = self.extract_images_from_html(html_content, url)
                
                # Tr√≠ch xu·∫•t video t·ª´ n·ªôi dung b√†i vi·∫øt
                videos = self.extract_videos_from_html(html_content)
                
            except Exception as e:
                print(f"  L·ªói t·∫£i HTML: {e}")
        
        # T·∫£i ·∫£nh t·ª´ n·ªôi dung b√†i vi·∫øt
        downloaded_images = []
        
        if images:
            print(f"  T√¨m th·∫•y {len(images)} ·∫£nh trong n·ªôi dung b√†i vi·∫øt")
            images_dir = os.path.join(article_dir, 'images')
            os.makedirs(images_dir, exist_ok=True)
            
            for i, img in enumerate(images):
                img_url = img.get('url', '')
                img_alt = img.get('alt', f'image_{i}')
                
                if img_url:
                    # T·∫°o t√™n file
                    filename = self.sanitize_filename(img_alt)
                    if not filename:
                        filename = f'image_{i:03d}'
                    
                    # L·∫•y extension t·ª´ URL
                    parsed_url = urlparse(img_url)
                    path = parsed_url.path
                    if '.' in path:
                        ext = path.split('.')[-1].split('?')[0]
                        if ext.lower() in ['jpg', 'jpeg', 'png', 'gif', 'webp']:
                            filename += f'.{ext}'
                        else:
                            filename += '.jpg'
                    else:
                        filename += '.jpg'
                    
                    filepath = os.path.join(images_dir, filename)
                    
                    print(f"    T·∫£i ·∫£nh {i+1}/{len(images)}: {filename}")
                    if self.download_file(img_url, filepath):
                        downloaded_images.append({
                            'original_url': img_url,
                            'alt': img_alt,
                            'local_path': filepath,
                            'filename': filename
                        })
                    time.sleep(0.5)  # Delay ƒë·ªÉ tr√°nh spam
        
        # X·ª≠ l√Ω video t·ª´ n·ªôi dung b√†i vi·∫øt
        if videos:
            print(f"  T√¨m th·∫•y {len(videos)} video trong n·ªôi dung b√†i vi·∫øt")
            videos_dir = os.path.join(article_dir, 'videos')
            os.makedirs(videos_dir, exist_ok=True)
            
            for i, video_url in enumerate(videos):
                print(f"    Video {i+1}: {video_url}")
                
                # L∆∞u th√¥ng tin video
                video_info = {
                    'url': video_url,
                    'index': i,
                    'type': 'video'
                }
                
                with open(os.path.join(videos_dir, f'video_{i:03d}_info.json'), 'w', encoding='utf-8') as f:
                    json.dump(video_info, f, ensure_ascii=False, indent=2)
        
        # T·∫°o b√°o c√°o
        report = {
            'article_title': title,
            'processed_at': datetime.now().isoformat(),
            'images_found': len(images),
            'images_downloaded': len(downloaded_images),
            'videos_found': len(videos),
            'article_dir': article_dir
        }
        
        with open(os.path.join(article_dir, 'download_report.json'), 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        
        print(f"  ‚úÖ Ho√†n th√†nh: {len(downloaded_images)} ·∫£nh, {len(videos)} video")
        return report
    
    def process_all_articles(self):
        """X·ª≠ l√Ω t·∫•t c·∫£ b√†i vi·∫øt"""
        try:
            with open(self.json_file, 'r', encoding='utf-8') as f:
                articles = json.load(f)
            
            print(f"T√¨m th·∫•y {len(articles)} b√†i vi·∫øt trong {self.json_file}")
            
            total_reports = []
            for i, article in enumerate(articles, 1):
                report = self.process_article(article, i)
                total_reports.append(report)
                time.sleep(1)  # Delay gi·ªØa c√°c b√†i vi·∫øt
            
            # T·∫°o b√°o c√°o t·ªïng h·ª£p
            summary = {
                'total_articles': len(articles),
                'processed_at': datetime.now().isoformat(),
                'total_images_found': sum(r['images_found'] for r in total_reports),
                'total_images_downloaded': sum(r['images_downloaded'] for r in total_reports),
                'total_videos_found': sum(r['videos_found'] for r in total_reports),
                'base_directory': self.base_dir,
                'reports': total_reports
            }
            
            with open(os.path.join(self.base_dir, 'summary_report.json'), 'w', encoding='utf-8') as f:
                json.dump(summary, f, ensure_ascii=False, indent=2)
            
            print(f"\nüéâ HO√ÄN TH√ÄNH!")
            print(f"üìÅ Th∆∞ m·ª•c: {self.base_dir}")
            print(f"üìä T·ªïng c·ªông: {len(articles)} b√†i vi·∫øt")
            print(f"üñºÔ∏è  ·∫¢nh: {summary['total_images_downloaded']}/{summary['total_images_found']}")
            print(f"üé• Video: {summary['total_videos_found']}")
            
        except Exception as e:
            print(f"L·ªói: {e}")

if __name__ == "__main__":
    downloader = MediaDownloader()
    downloader.process_all_articles() 